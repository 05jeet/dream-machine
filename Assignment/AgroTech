
# importing necessary packages
import pandas as pd
import numpy as np
import seaborn as sns
# setting dinnensions for plot
sns.set(rc={'figure.figsize' : (11.7,8.27)})
# checking the working directory
import os
os. getcwd()
df=pd.read_excel("/Users/addynobi/Documents/Essex Jan 22/CE 888 7 SP/Assignment/Data10.xlsx")
df.shape
df.nunique()
df.isnull().sum()
df.describe()
#df.rename( columns={'Plant Date':'Plant_Date'}, inplace=True )
df
df.sort_values('Batch Number')
df.rename( columns={'Batch Number':'Planting_Batch'}, inplace=True )
df.info()
planting=pd.read_excel("/Users/addynobi/Documents/Essex Jan 22/CE 888 7 SP/Assignment/Data10.xlsx", sheet_name='planting')
planting.nunique()
planting.info()
# we found that date is object data type in planting tab.
planting['Plant_Date']=pd.to_datetime(planting['Plant_Date'])
# got out of bounds error
planting['Plant_Date'] = pd.to_datetime(planting['Plant_Date'], errors = 'coerce')

#==============================================================
# Filling missing values in Plant Date column through Plant_Date column from planting sheet
df['Plant Date']. fillna(planting['Plant_Date'], inplace=True)

# Filling missing values in Flight Date column from Flight Dates Tab
fl_date=pd.read_excel("/Users/addynobi/Documents/Essex Jan 22/CE 888 7 SP/Assignment/Data10.xlsx", sheet_name='flight dates')
fl_date.rename( columns={'Flight Date':'Flight_Date'}, inplace=True )
fl_date.rename( columns={'Batch Number':'Planting_Batch'}, inplace=True )
fl_merge= pd.merge(df, fl_date, on='Planting_Batch', how='outer')
fl_merge.info()
fl_date.info()
fl_date.nunique()
df['Flight Date']. fillna(fl_merge['Flight_Date'], inplace=True)
df.info()
df.isnull().sum()
df.to_excel("try_cream.xlsx")
# We now have 0 null values in the Flight Date Column
data_merge= pd.merge(df, planting, on='Planting_Batch', how='inner')
data_merge.isnull().sum()


#=======================================================
#Merging the Weather Data
weather=pd.read_excel("/Users/addynobi/Documents/Essex Jan 22/CE 888 7 SP/Assignment/Data10.xlsx", sheet_name='weather')
weather.rename( columns={'Unnamed: 0':'Plant Date'}, inplace=True )
weather.describe()
weather.info()
start_date="2020-04-11 00:00:00"
end_date="2020-08-27 00:00:00"
mask=(weather['Plant Date'] >= start_date) & (weather['Plant Date'] <= end_date)
weather_n=weather.loc[mask]
data_merge_col = pd.merge(df, weather_n, on='Plant Date')
data_merge_col.to_excel("ice_cream.xlsx")
data_merge_col['Class'].astype('str')
data_merge_col.info()
data_merge_col.describe()
data_merge_col.dtypes()
# DATA CLEANING ======================================================
# Now we have got the merged data. Next step is to check description of the new dataframe, check null values and see which columns are irrelevant to delete.
# Class variable has been classified as int actually it is object data. So we will deal with it and make it object.
# We got to check possible imputations for missing values.
# After this we check skewness in each variable and see how balanced are they. 
data_merge_col.describe()
data_merge_col.info()
data_merge_col.isnull().sum()
# Leaves column and Remove column were found to have more than 4759 null values. So we can delete them.
col=['Leaves', 'Remove'] 
data_prep=data_merge_col.drop(columns=col, axis=1)
data_prep.info()
data_prep['Age'] = (data_prep['Check Date']-data_prep['Plant Date']).dt.days
data_prep.isnull().sum()

# Placing mean values in place of nan values
data_prep['Head Weight (g)'].fillna(data_prep['Head Weight (g)'].mean(), inplace = True)
data_prep['Radial Diameter (mm)'].fillna(data_prep['Radial Diameter (mm)'].mean(), inplace = True)
data_prep['Polar Diameter (mm)'].fillna(data_prep['Polar Diameter (mm)'].mean(), inplace = True)
data_prep['Diameter Ratio'].fillna(data_prep['Diameter Ratio'].mean(), inplace = True)
data_prep['Fresh Weight (g)'].fillna(data_prep['Fresh Weight (g)'].mean(), inplace = True)
data_prep['Leaf Area (cm^2)'].fillna(data_prep['Leaf Area (cm^2)'].mean(), inplace = True)
data_prep['Density (kg/L)'].fillna(data_prep['Density (kg/L)'].mean(), inplace = True)

# there are now zero nan values in the dataframe

#Trash Code==================================================
#f_data=data_merge_col
#f_data.pop('Leaves')
#f_data.pop('Flight Date')
#f_data.isnull().sum()
#sns.displot(f_data['Fresh Weight (g)'])
#sns.displot(f_data['Head Weight (g)']) # this variable is highly skewed
#sns.displot(f_data['Radial Diameter (mm)']) # somewhat skewed
#sns.displot(f_data['Polar Diameter (mm)' ])# not so skewed

# skewness in these variables could be beacuse of missing values.
#============================================================
#Set dependant and independant variables
#Analyize which variables have strong correlation with target variables
# Encode Class data type 
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import OneHotEncoder

#creating instance of one-hot-encoder
encoder = OneHotEncoder(handle_unknown='ignore')

#perform one-hot encoding on 'team' column 
encoder_data_prep = pd.DataFrame(encoder.fit_transform(data_prep[['Class']]).toarray())

#merge one-hot encoded columns back with original DataFrame
final_df = data_prep.join(encoder_data_prep)
final_df.corr 
corrMatrix = final_df.corr()
sns.heatmap(corrMatrix, annot=True)
# it is evident from the heatmap that target variables: head weight, polar diameter and radial diameter
# has good correlation with only: Leaf Area, Solar Radiation, Wind Speed, Battery Voltage, Age, Precipitation.
# has negetive correlation with temp, humidity and dew point, variables.
# so we will delete other variables which have no impact windspeed max, leaf wetness time, ETO, class
col1=['Class','Plant Date', 'Check Date','Wind Speed [max]', 'Leaf Wetness [time]', 'ET0 [result]' ]
df_reg=final_df.drop(columns=col1, axis=1)
df_reg.info()

#Performing Regression=========================
y=df_reg[['Head Weight (g)', 'Radial Diameter (mm)', 'Polar Diameter (mm)']]
X1=df_reg[['Fresh Weight (g)', 'Diameter Ratio', 'Density (kg/L)', 'Leaf Area (cm^2)', 'Square ID', 'Solar Radiation [avg]','Precipitation [sum]', 'Wind Speed [avg]', 'Battery Voltage [last]', 'Air Temperature [avg]', 'Air Temperature [max]', 'Air Temperature [min]', 'Relative Humidity [avg]', 'Dew Point [avg]', 'Dew Point [min]', 'Age' ]]
X1.isnull().sum()
# VIF and multicollinearity
from statsmodels.stats.outliers_influence import variance_inflation_factor
features=['Fresh Weight (g)', 'Diameter Ratio', 'Density (kg/L)', 'Leaf Area (cm^2)', 'Square ID', 'Solar Radiation [avg]','Precipitation [sum]', 'Wind Speed [avg]', 'Battery Voltage [last]', 'Air Temperature [avg]', 'Air Temperature [max]', 'Air Temperature [min]', 'Relative Humidity [avg]', 'Dew Point [avg]', 'Dew Point [min]', 'Age' ]
vif = pd.DataFrame()
vif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]
vif['features'] = X.columns
# from vif rable we get high values for features: Battery Voltage (last), Air Temperature [avg], Relative Humidity [avg] and Dew Point [avg]
# we can remove these 4 columns
col2=['Battery Voltage [last]', 'Air Temperature [avg]', 'Relative Humidity [avg]' ,'Dew Point [avg]' ]
X=X1.drop(columns=col2, axis=1)

#FINAL dataframes for multi output regression=========================
#dependent variables in df y
#independant variables in df X
from sklearn.linear_model import LinearRegression
model = LinearRegression()
# fit model
model.fit(X, y)
# make a prediction
row = [15, 1.0377, 21.730, 185, 208, 251, 0, 1.4, 24.45, 4.42, 4.3, 19 ]
yhat = model.predict([row])
# summarize prediction
print(yhat[0])
# we are getting predicted values quite away from actual
#KNN Model==========================================
from sklearn.neighbors import KNeighborsRegressor
model = KNeighborsRegressor()
# fit model
model.fit(X, y)
# make a prediction
row = [15, 1.0377, 21.730, 185, 208, 251, 0, 1.4, 24.45, 4.42, 4.3, 19 ]
yhat = model.predict([row])
# summarize prediction
print(yhat[0])
# we are getting predicted values quite close to actual values

#Decision Tree Multioutput Regression==============
from sklearn.tree import DecisionTreeRegressor
model = DecisionTreeRegressor()
# fit model
model.fit(X, y)
# make a prediction
row = [15, 1.0377, 21.730, 185, 208, 251, 0, 1.4, 24.45, 4.42, 4.3, 19 ]
yhat = model.predict([row])
# summarize prediction
print(yhat[0])
# Decision Tree model is going very good accuracy

#Cross Validation============================================
#Evaluate Multioutput Regression With Cross-Validation

# evaluate multioutput regression model with k-fold cross-validation
from numpy import absolute
from numpy import mean
from numpy import std
from sklearn.datasets import make_regression
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import RepeatedKFold
model = DecisionTreeRegressor()
# define the evaluation procedure
cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)
# evaluate the model and collect the scores
n_scores = cross_val_score(model, X, y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)
# force the scores to be positive
n_scores = absolute(n_scores)
# summarize performance
print('MAE: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))

from sklearn.datasets import make_regression
from sklearn.multioutput import MultiOutputRegressor
from sklearn.svm import SVR
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error

# SVMRegressor Methodology========================================
# Train/test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=33)

# Create the SVR regressor
svr = SVR(epsilon=0.2)

# Create the Multioutput Regressor
mor = MultiOutputRegressor(svr)

# Train the regressor
mor = mor.fit(X_train, y_train)

# Generate predictions for testing data
y_pred = mor.predict(X_test)

#SVR model was attempted but the preb=vious models worked better.

